{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<h2 align=\"center\">Spacy Tokenization</h2>"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T13:44:52.032837Z",
     "start_time": "2024-06-03T13:44:51.401883Z"
    }
   },
   "source": [
    "import spacy"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Crear un objeto lingüístico en blanco y tokenizar las palabras de una frase"
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-06-03T13:44:53.345086Z",
     "start_time": "2024-06-03T13:44:53.234706Z"
    }
   },
   "source": [
    "# Al crear un objeto de lenguaje vacío se obtiene un tokenizador y un canal vacío.\n",
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "doc = nlp(\"Dr. Simi es un personaje le encanta comer tacos de canasta, ya que cuestan $10.00 pesos.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Simi\n",
      "es\n",
      "un\n",
      "personaje\n",
      "le\n",
      "encanta\n",
      "comer\n",
      "tacos\n",
      "de\n",
      "canasta\n",
      ",\n",
      "ya\n",
      "que\n",
      "cuestan\n",
      "$\n",
      "10.00\n",
      "pesos\n",
      ".\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<h3>Utilizar el índice para capturar tokens</h3>"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T13:46:07.591204Z",
     "start_time": "2024-06-03T13:46:07.585891Z"
    }
   },
   "source": [
    "# Se obtiene el primer token de la frase\n",
    "doc[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dr."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-06-03T13:46:08.861883Z",
     "start_time": "2024-06-03T13:46:08.854681Z"
    }
   },
   "source": [
    "# Se obtiene el indice 1 de la frase\n",
    "token = doc[1]\n",
    "token.text"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Simi'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T13:46:10.630797Z",
     "start_time": "2024-06-03T13:46:10.624540Z"
    }
   },
   "source": [
    "# dir(token) muestra los atributos y métodos del token\n",
    "dir(token)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T13:46:12.060429Z",
     "start_time": "2024-06-03T13:46:12.054612Z"
    }
   },
   "source": [
    "# type/nlp) muestra el tipo de objeto del nlp\n",
    "type(nlp)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.es.Spanish"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T13:46:13.238202Z",
     "start_time": "2024-06-03T13:46:13.232226Z"
    }
   },
   "source": [
    "# type(doc) muestra el tipo de objeto del documento\n",
    "type(doc)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T13:46:13.816355Z",
     "start_time": "2024-06-03T13:46:13.810320Z"
    }
   },
   "source": [
    "# type(token) muestra el tipo de objeto del token\n",
    "type(token)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-06-03T14:09:22.221527Z",
     "start_time": "2024-06-03T14:09:22.214918Z"
    }
   },
   "source": [
    "# nlp.pipe_names muestra los nombres de los componentes del pipeline\n",
    "nlp.pipe_names"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 81
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<h3>Objeto span</h3>"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T14:09:41.399752Z",
     "start_time": "2024-06-03T14:09:41.394086Z"
    }
   },
   "source": [
    "span = doc[0:5]\n",
    "span"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Aguachile es un platillo"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T13:46:41.475480Z",
     "start_time": "2024-06-03T13:46:41.469874Z"
    }
   },
   "source": "type(span)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<h3>Atributos de los tokens</h3>"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T13:58:31.113177Z",
     "start_time": "2024-06-03T13:58:31.108094Z"
    }
   },
   "source": "doc = nlp(\"Simi le dio $500 a su amigo para comprar tacos.\")",
   "outputs": [],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T13:58:31.251915Z",
     "start_time": "2024-06-03T13:58:31.246904Z"
    }
   },
   "source": [
    "# Aqui se obtiene el primer token de la frase (Simi) Token0\n",
    "token0 = doc[0]\n",
    "token0"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Simi"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T13:58:31.421674Z",
     "start_time": "2024-06-03T13:58:31.416050Z"
    }
   },
   "source": [
    "# Es Alfabético el token0 (Simi)?\n",
    "token0.is_alpha"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-06-03T13:58:31.578989Z",
     "start_time": "2024-06-03T13:58:31.574359Z"
    }
   },
   "source": [
    "# Es un número el token0 (Simi)?\n",
    "token0.like_num"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T14:01:12.559508Z",
     "start_time": "2024-06-03T14:01:12.554156Z"
    }
   },
   "source": [
    "# Obtenemos el Token3 de la frase ($)\n",
    "token3 = doc[3]\n",
    "token3"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T14:01:12.833732Z",
     "start_time": "2024-06-03T14:01:12.828668Z"
    }
   },
   "source": [
    "# Es un número el token3 ($)?\n",
    "token3.like_num"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T14:01:13.161545Z",
     "start_time": "2024-06-03T14:01:13.156037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Es una moneda el token3 ($)?\n",
    "token3.is_currency"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T14:01:22.044839Z",
     "start_time": "2024-06-03T14:01:22.038137Z"
    }
   },
   "source": [
    "# Obtenemos el Token4 de la frase (500)\n",
    "token4 = doc[4]\n",
    "token4"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T14:01:22.922361Z",
     "start_time": "2024-06-03T14:01:22.917349Z"
    }
   },
   "source": "token4.like_num",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T14:01:24.336173Z",
     "start_time": "2024-06-03T14:01:24.330931Z"
    }
   },
   "source": "token4.is_currency",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-06-03T14:04:29.978453Z",
     "start_time": "2024-06-03T14:04:29.972632Z"
    }
   },
   "source": [
    "for token in doc:\n",
    "    print(token, \"==>\", \"index: \", token.i, \"is_alpha (Alfabético):\", token.is_alpha, \n",
    "          \", is_punct (Puntuación):\", token.is_punct, \n",
    "          \", like_num (Numero):\", token.like_num,\n",
    "          \", is_currency (Divisa/Moneda):\", token.is_currency, \"\\n\"\n",
    "         )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simi ==> index:  0 is_alpha (Alfabético): True , is_punct (Puntuación): False , like_num (Numero): False , is_currency (Divisa/Moneda): False \n",
      "\n",
      "le ==> index:  1 is_alpha (Alfabético): True , is_punct (Puntuación): False , like_num (Numero): False , is_currency (Divisa/Moneda): False \n",
      "\n",
      "dio ==> index:  2 is_alpha (Alfabético): True , is_punct (Puntuación): False , like_num (Numero): False , is_currency (Divisa/Moneda): False \n",
      "\n",
      "$ ==> index:  3 is_alpha (Alfabético): False , is_punct (Puntuación): False , like_num (Numero): False , is_currency (Divisa/Moneda): True \n",
      "\n",
      "500 ==> index:  4 is_alpha (Alfabético): False , is_punct (Puntuación): False , like_num (Numero): True , is_currency (Divisa/Moneda): False \n",
      "\n",
      "a ==> index:  5 is_alpha (Alfabético): True , is_punct (Puntuación): False , like_num (Numero): False , is_currency (Divisa/Moneda): False \n",
      "\n",
      "su ==> index:  6 is_alpha (Alfabético): True , is_punct (Puntuación): False , like_num (Numero): False , is_currency (Divisa/Moneda): False \n",
      "\n",
      "amigo ==> index:  7 is_alpha (Alfabético): True , is_punct (Puntuación): False , like_num (Numero): False , is_currency (Divisa/Moneda): False \n",
      "\n",
      "para ==> index:  8 is_alpha (Alfabético): True , is_punct (Puntuación): False , like_num (Numero): False , is_currency (Divisa/Moneda): False \n",
      "\n",
      "comprar ==> index:  9 is_alpha (Alfabético): True , is_punct (Puntuación): False , like_num (Numero): False , is_currency (Divisa/Moneda): False \n",
      "\n",
      "tacos ==> index:  10 is_alpha (Alfabético): True , is_punct (Puntuación): False , like_num (Numero): False , is_currency (Divisa/Moneda): False \n",
      "\n",
      ". ==> index:  11 is_alpha (Alfabético): False , is_punct (Puntuación): True , like_num (Numero): False , is_currency (Divisa/Moneda): False \n",
      "\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": " <h3>Personalización del tokenizador</h3>"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T14:05:08.810451Z",
     "start_time": "2024-06-03T14:05:08.763404Z"
    }
   },
   "source": [
    "# ORTH es un atributo que se utiliza para personalizar el tokenizador\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp = spacy.blank(\"es\")\n",
    "doc = nlp(\"Aguachile es un platillo un poco picoso\")\n",
    "\n",
    "# Aquí se obtiene los tokens de la frase\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aguachile', 'es', 'un', 'platillo', 'un', 'poco', 'picoso']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-06-03T14:05:37.504161Z",
     "start_time": "2024-06-03T14:05:37.497151Z"
    }
   },
   "source": [
    "# Se personaliza el tokenizador para que divida la palabra Aguachile en dos palabras Agua y chile\n",
    "nlp.tokenizer.add_special_case(\"Aguachile\", [\n",
    "    {ORTH: \"Agua\"},\n",
    "    {ORTH: \"chile\"},\n",
    "])\n",
    "\n",
    "# Se tokeniza nuevamente la frase\n",
    "doc = nlp(\"Aguachile es un platillo un poco picoso\")\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Agua', 'chile', 'es', 'un', 'platillo', 'un', 'poco', 'picoso']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<h3>Tokenización o segmentación de frases</h3>"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T14:12:01.167913Z",
     "start_time": "2024-06-03T14:12:01.162242Z"
    }
   },
   "source": "nlp.pipeline",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x1d31844dc50>)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T14:14:12.021451Z",
     "start_time": "2024-06-03T14:14:12.016746Z"
    }
   },
   "source": [
    "# nlp.add_pipe('sentencizer') se agrega un componente al pipeline para poder segmentar las frases\n",
    "try:\n",
    "    nlp.add_pipe('sentencizer')\n",
    "except ValueError:\n",
    "    print(\"El componente ya está en el pipeline\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El componente ya está en el pipeline\n"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T14:14:00.341089Z",
     "start_time": "2024-06-03T14:14:00.336611Z"
    }
   },
   "source": [
    "doc = nlp(\"Dr. Simi le gusta comer tacos de canasta. Los tacos de canasta cuestan $10.00 pesos cada uno.\")\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Simi le gusta comer tacos de canasta.\n",
      "Los tacos de canasta cuestan $10.00 pesos cada uno.\n"
     ]
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T14:14:21.767690Z",
     "start_time": "2024-06-03T14:14:21.762588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Simi\n",
      "le\n",
      "gusta\n",
      "comer\n",
      "tacos\n",
      "de\n",
      "canasta\n",
      ".\n",
      "Los\n",
      "tacos\n",
      "de\n",
      "canasta\n",
      "cuestan\n",
      "$\n",
      "10.00\n",
      "pesos\n",
      "cada\n",
      "uno\n",
      ".\n"
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
